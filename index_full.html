<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Shuo Zhang.GitHub.io : personal web">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Mamamya88.GitHub.io</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/mamamya88">View on GitHub</a>

          <h1 id="project_title">Light Field Image Processing </h1>
          <h2 id="project_tagline">Shuo Zhang - Beihang University</h2>

        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h3>
<a id="welcome-to-github-pages" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Robust Depth Estimation for Light Field via Spinning Parallelogram Operator</h3>
<p>Shuo Zhang, Hao Sheng, Chao Li, Jun Zhang, Zhang Xiong, 
  Computer Vision and Image Understanding, Volume 145 Issue C, April 2016</p>
<p>Removing the influence of occlusion on the depth estimation for light field images has always been a difficult problem, especially for highly noisy and aliased images captured by plenoptic cameras.A spinning parallelogram operator (SPO) is integrated into a depth estimation framework to solve these problems. Utilizing the regions divided by the operator in an Epipolar Plane Image (EPI), the lines that indicate depth information are located by maximizing the distribution distances of the regions. Unlike traditional multi-view stereo matching methods, the distance measure is able to keep the correct depth information even if they are occluded or noisy. We further choose the relative reliable information among the rich structures in the light field to reduce the influences of occlusion and ambiguity. The discrete labeling problem is then solved by a filter-based algorithm to fast approximate the optimal solution. The major advantage of the proposed method is that it is insensitive to occlusion, noise, and aliasing, and has no requirement for depth range and angular resolution. It therefore can be used in various light field images, especially in plenoptic camera images. Experimental results demonstrate that the proposed method outperforms state-of-the-art depth estimation methods on light field images, including both real world images and synthetic images, especially near occlusion boundaries.</p>
<p> <a href="http://www.sciencedirect.com/science/article/pii/S1077314215002714"> Paper </a> | Code | Bibtex</p>

<h3>
<a id="designer-templates" class="anchor" href="#designer-templates" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Guided Integral Filter for Light Field Stereo Matching</h3>
<p>Hao Sheng, Shuo Zhang, Gengliang Zhu, Zhang Xiong, IEEE International Conference on Image Processing (ICIP), 2015</p>
<p>Different from the traditional multi-view images, the sam- pling in angular resolution of light field images is continu- ous in each direction. Therefore, an angular sampling image, comprising of matching points extracted from each view, can be constructed at different possible depth for each point. In this paper, we prove that the angular sampling image of an oc- cluded point at the correct depth is similar to the scene around the point. On this basis, a guided integral filter acquired from the reference view is proposed to weight the matching points for consistency measure, which predicts the probabilities of occlusions. The discrete labeling problem is then solved by a filter-based algorithm to approximate the optimal solution. Experimental results demonstrate that the proposed method outperforms the state-of-the-art methods for light field depth estimation both in occluded and ambiguous regions, and has no requirement for angular resolution.</p>
<p> <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=7350920&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D7350920"> Paper </a> | Code | Bibtex</p>

<h3>
<a id="creating-pages-manually" class="anchor" href="#creating-pages-manually" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Relative Location for Light Field Saliency Detection</h3>
<p>Hao Sheng, Shuo Zhang, Xiaoyu Liu, Zhang Xiong, IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2016</p>
<p>Light field images, which capture multiple images from different angles of a scene, have been proved that can detect salient regions more effectively. Instead of estimating depth labels from light field images, we proposed to extract relative locations, which can distinguish whether the object is located before the focus plane of the main lens or not, for saliency detection. The relative locations are calculated by compar- ing raw light field images captured by plenoptic cameras and central views of scenes. The relative locations are then inte- grated to a modified saliency detection framework to obtain the salient regions. Experimental results demonstrate that the proposed relative locations can help to improve the accuracy of results, which is also efficient. Moreover, the modified framework outperforms the state-of-the-art methods for light field images saliency detection.</p>
<p> Paper | Code | Bibtex </p>

      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
